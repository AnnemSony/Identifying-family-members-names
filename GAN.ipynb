{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58d9e23",
   "metadata": {},
   "source": [
    "Generator generate the fake image from noise and descriminator used to check eihet the image is real or fake.\n",
    "Generator we perform upsample ane in discriminator we perform normal classiifcation(downsample).\n",
    "CONV2DTranspose is as upsampling image.\n",
    "we need two loss functions:\n",
    "1. for generator ( ccross entropy-\n",
    "2.for descriminator(real output and fake output)\n",
    "\n",
    "we need two optimizers\n",
    "one Adam Optimizer to update the weights for generator and another one for descriminator.\n",
    "key items:\n",
    "Noise dimension\n",
    "to generate images\n",
    "we pass noise to generator then we pass both fakeotput and real output to discriminator .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c41d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 262 files belonging to 17 classes.\n",
      "Using 210 files for training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the dataset of face images\n",
    "faces_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "\"C:/Users/annem/OneDrive/Desktop/cnn/train/\",\n",
    "validation_split=0.2,\n",
    "subset=\"training\",\n",
    "seed=42,\n",
    "image_size=(64, 64),\n",
    "batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f8fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f81e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d9978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the generator model\n",
    "def build_generator(latent_dim):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(8 * 8 * 256, input_dim=latent_dim))\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Conv2D(3, (3, 3), activation=\"tanh\", padding=\"same\"))\n",
    "    return model\n",
    "\n",
    "# Define the discriminator model\n",
    "def build_discriminator(img_shape):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\", input_shape=img_shape))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "# Define the GAN model\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = keras.Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "# Define the loss functions\n",
    "cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "# Define the optimizer\n",
    "generator_optimizer = keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# Define the training loop\n",
    "latent_dim = 100\n",
    "epochs = 50\n",
    "\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator((64, 64, 3))\n",
    "gan = build_gan(generator, discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579ce2fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Batch 0/7 - Gen loss: 0.5297284126281738 - Disc loss: 6.810472011566162\n",
      "Epoch 2/50\n",
      "Batch 0/7 - Gen loss: 0.4663059711456299 - Disc loss: 9.340594291687012\n",
      "Epoch 3/50\n",
      "Batch 0/7 - Gen loss: 0.4445614516735077 - Disc loss: 6.74867057800293\n",
      "Epoch 4/50\n",
      "Batch 0/7 - Gen loss: 0.3661104738712311 - Disc loss: 11.252571105957031\n",
      "Epoch 5/50\n",
      "Batch 0/7 - Gen loss: 0.3609847128391266 - Disc loss: 7.508737564086914\n",
      "Epoch 6/50\n",
      "Batch 0/7 - Gen loss: 0.36988407373428345 - Disc loss: 4.344683647155762\n",
      "Epoch 7/50\n",
      "Batch 0/7 - Gen loss: 0.34205976128578186 - Disc loss: 8.166535377502441\n",
      "Epoch 8/50\n",
      "Batch 0/7 - Gen loss: 0.34578636288642883 - Disc loss: 7.831428050994873\n",
      "Epoch 9/50\n",
      "Batch 0/7 - Gen loss: 0.31943339109420776 - Disc loss: 4.287395477294922\n",
      "Epoch 10/50\n",
      "Batch 0/7 - Gen loss: 0.3161329925060272 - Disc loss: 9.522903442382812\n",
      "Epoch 11/50\n",
      "Batch 0/7 - Gen loss: 0.311786949634552 - Disc loss: 9.112005233764648\n",
      "Epoch 12/50\n",
      "Batch 0/7 - Gen loss: 0.28062257170677185 - Disc loss: 8.304296493530273\n",
      "Epoch 13/50\n",
      "Batch 0/7 - Gen loss: 0.27881529927253723 - Disc loss: 7.747394561767578\n",
      "Epoch 14/50\n",
      "Batch 0/7 - Gen loss: 0.26427385210990906 - Disc loss: 9.779052734375\n",
      "Epoch 15/50\n",
      "Batch 0/7 - Gen loss: 0.24909915030002594 - Disc loss: 6.660178184509277\n",
      "Epoch 16/50\n",
      "Batch 0/7 - Gen loss: 0.20600801706314087 - Disc loss: 6.443789005279541\n",
      "Epoch 17/50\n",
      "Batch 0/7 - Gen loss: 0.21797806024551392 - Disc loss: 8.328083038330078\n",
      "Epoch 18/50\n",
      "Batch 0/7 - Gen loss: 0.20462125539779663 - Disc loss: 6.800206184387207\n",
      "Epoch 19/50\n",
      "Batch 0/7 - Gen loss: 0.17320962250232697 - Disc loss: 8.727466583251953\n",
      "Epoch 20/50\n",
      "Batch 0/7 - Gen loss: 0.16917650401592255 - Disc loss: 8.503408432006836\n",
      "Epoch 21/50\n",
      "Batch 0/7 - Gen loss: 0.1723400503396988 - Disc loss: 9.627023696899414\n",
      "Epoch 22/50\n",
      "Batch 0/7 - Gen loss: 0.14658689498901367 - Disc loss: 10.544271469116211\n",
      "Epoch 23/50\n",
      "Batch 0/7 - Gen loss: 0.13914644718170166 - Disc loss: 9.342329025268555\n",
      "Epoch 24/50\n",
      "Batch 0/7 - Gen loss: 0.12476728111505508 - Disc loss: 13.615480422973633\n",
      "Epoch 25/50\n",
      "Batch 0/7 - Gen loss: 0.10911165177822113 - Disc loss: 8.898972511291504\n",
      "Epoch 26/50\n",
      "Batch 0/7 - Gen loss: 0.10744792222976685 - Disc loss: 8.476070404052734\n",
      "Epoch 27/50\n",
      "Batch 0/7 - Gen loss: 0.10288427770137787 - Disc loss: 7.5582427978515625\n",
      "Epoch 28/50\n",
      "Batch 0/7 - Gen loss: 0.10382080078125 - Disc loss: 10.256010055541992\n",
      "Epoch 29/50\n",
      "Batch 0/7 - Gen loss: 0.08283863216638565 - Disc loss: 13.489253044128418\n",
      "Epoch 30/50\n",
      "Batch 0/7 - Gen loss: 0.09114664047956467 - Disc loss: 11.158239364624023\n",
      "Epoch 31/50\n",
      "Batch 0/7 - Gen loss: 0.08619870990514755 - Disc loss: 9.741697311401367\n",
      "Epoch 32/50\n",
      "Batch 0/7 - Gen loss: 0.06784634292125702 - Disc loss: 12.88178825378418\n",
      "Epoch 33/50\n",
      "Batch 0/7 - Gen loss: 0.06650928407907486 - Disc loss: 11.35131549835205\n",
      "Epoch 34/50\n",
      "Batch 0/7 - Gen loss: 0.07143327593803406 - Disc loss: 13.59420108795166\n",
      "Epoch 35/50\n",
      "Batch 0/7 - Gen loss: 0.06234215199947357 - Disc loss: 5.6927995681762695\n",
      "Epoch 36/50\n",
      "Batch 0/7 - Gen loss: 0.05399378389120102 - Disc loss: 9.904060363769531\n",
      "Epoch 37/50\n",
      "Batch 0/7 - Gen loss: 0.05488968640565872 - Disc loss: 6.800065517425537\n",
      "Epoch 38/50\n",
      "Batch 0/7 - Gen loss: 0.04982750862836838 - Disc loss: 12.748018264770508\n",
      "Epoch 39/50\n",
      "Batch 0/7 - Gen loss: 0.05077213793992996 - Disc loss: 10.981644630432129\n",
      "Epoch 40/50\n",
      "Batch 0/7 - Gen loss: 0.04674805700778961 - Disc loss: 11.45102310180664\n",
      "Epoch 41/50\n",
      "Batch 0/7 - Gen loss: 0.04144780710339546 - Disc loss: 10.040342330932617\n",
      "Epoch 42/50\n",
      "Batch 0/7 - Gen loss: 0.04585401341319084 - Disc loss: 9.731947898864746\n",
      "Epoch 43/50\n",
      "Batch 0/7 - Gen loss: 0.04018089547753334 - Disc loss: 12.13613510131836\n",
      "Epoch 44/50\n",
      "Batch 0/7 - Gen loss: 0.0398123636841774 - Disc loss: 10.023835182189941\n",
      "Epoch 45/50\n",
      "Batch 0/7 - Gen loss: 0.03643914312124252 - Disc loss: 10.509615898132324\n",
      "Epoch 46/50\n",
      "Batch 0/7 - Gen loss: 0.03579595312476158 - Disc loss: 10.630215644836426\n",
      "Epoch 47/50\n",
      "Batch 0/7 - Gen loss: 0.03621324151754379 - Disc loss: 8.180940628051758\n",
      "Epoch 48/50\n",
      "Batch 0/7 - Gen loss: 0.03430234268307686 - Disc loss: 12.154574394226074\n",
      "Epoch 49/50\n",
      "Batch 0/7 - Gen loss: 0.03421887755393982 - Disc loss: 15.723371505737305\n",
      "Epoch 50/50\n",
      "Batch 0/7 - Gen loss: 0.028803402557969093 - Disc loss: 14.14734935760498\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_size = 64\n",
    "#print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for i, batch in enumerate(faces_dataset):\n",
    "       # print(i)\n",
    "        #print(len(batch))\n",
    "        noise = tf.random.normal([batch[0].shape[0], latent_dim])\n",
    "        # Generate fake images\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = generator(noise, training=True)\n",
    "            fake_output = discriminator(fake_images, training=True)\n",
    "            gen_loss = generator_loss(fake_output)\n",
    "        gradients_of_generator = tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            real_output = discriminator(batch[0], training=True)\n",
    "            fake_output = discriminator(fake_images, training=True)\n",
    "            disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        gradients_of_discriminator = tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "        # Print loss values every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Batch {i}/{len(faces_dataset)} - Gen loss: {gen_loss} - Disc loss: {disc_loss}\")\n",
    "\n",
    "    # Generate a sample of fake images at different ages\n",
    "    for age in range(20, 80, 10):\n",
    "        # Generate random noise\n",
    "        noise = tf.random.normal([1, latent_dim])\n",
    "\n",
    "        # Generate a fake image at the specified age\n",
    "        fake_image = generator(noise, training=False)\n",
    "        fake_image = np.squeeze(fake_image.numpy())\n",
    "        fake_image = (fake_image + 1) / 2.0 \n",
    "        file_name=f\"C:/Users/annem/OneDrive/Desktop/{age}.png\"\n",
    "        plt.imsave(file_name, fake_image)\n",
    "#         fig, axs=plt.subplots(1, fake_image, figsize=(15,15))\n",
    "#         for i in range(fake_image):\n",
    "#             axs[i].imshow(fake_image[i])\n",
    "# #             axs[i].axis('off')\n",
    "# #         plt.show()\n",
    "        \n",
    "#         # Rescale pixel values to [0, 1]\n",
    "#         fake_image = cv2.cvtColor(fake_image, cv2.COLOR_RGB2BGR)  # Convert from RGB to BGR\n",
    "#         fake_image = (fake_image * 255).astype(np.uint8)  # Rescale pixel values to [0, 255]\n",
    "\n",
    "#         # Save the fake image\n",
    "#         cv2.imshow(f\"fake_image_{age}.jpg\", fake_image)\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece60d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7dcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38060b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e7317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a67d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02938f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03922e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
