{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3e190a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imgaug\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
      "     ------------------------------------- 948.0/948.0 kB 10.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Pillow in c:\\users\\annem\\anaconda3\\lib\\site-packages (from imgaug) (9.4.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from imgaug) (1.23.5)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\annem\\anaconda3\\lib\\site-packages (from imgaug) (4.7.0.72)\n",
      "Requirement already satisfied: imageio in c:\\users\\annem\\anaconda3\\lib\\site-packages (from imgaug) (2.26.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\annem\\anaconda3\\lib\\site-packages (from imgaug) (3.7.0)\n",
      "Requirement already satisfied: six in c:\\users\\annem\\anaconda3\\lib\\site-packages (from imgaug) (1.16.0)\n",
      "Collecting Shapely\n",
      "  Downloading shapely-2.0.1-cp39-cp39-win_amd64.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 12.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\annem\\anaconda3\\lib\\site-packages (from imgaug) (1.10.0)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from imgaug) (0.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (22.0)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.9.1-cp39-cp39-win_amd64.whl (38.6 MB)\n",
      "     --------------------------------------- 38.6/38.6 MB 25.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (2.8.4)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (2021.7.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from scikit-image>=0.14.2->imgaug) (0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from matplotlib->imgaug) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from matplotlib->imgaug) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from matplotlib->imgaug) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from matplotlib->imgaug) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from matplotlib->imgaug) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from matplotlib->imgaug) (1.0.5)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from matplotlib->imgaug) (5.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\annem\\anaconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->imgaug) (3.11.0)\n",
      "Installing collected packages: Shapely, scipy, imgaug\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.0\n",
      "    Uninstalling scipy-1.10.0:\n",
      "      Successfully uninstalled scipy-1.10.0\n",
      "Successfully installed Shapely-2.0.1 imgaug-0.4.0 scipy-1.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16fc255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\photos\\George Hanson\\Hanson George.jpg\")\n",
    "\n",
    "# Define augmentation sequence\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16 pixels\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Fliplr(1.0),\n",
    "   # iaa.Flipud(1.0),\n",
    "    # flip horizontally with probability 0.5\n",
    "    #iaa.GaussianBlur(sigma=(0, 3.0)), # apply gaussian blur with sigma between 0 and 3.0\n",
    "    #iaa.Dropout(p=(0, 0.2)), # randomly remove up to 20% of the pixels\n",
    "    #iaa.AdditiveGaussianNoise(scale=(0, 0.1*255)), # add gaussian noise with mean 0 and standard deviation between 0 and 0.1*255\n",
    "    iaa.LinearContrast((0.75, 1.5)), # adjust contrast by scaling each pixel value between 0.75 and 1.5\n",
    "], random_order=True) # apply augmentations in random order\n",
    "\n",
    "# Generate augmented images\n",
    "f=r\"C:\\Users\\annem\\OneDrive\\Desktop\\photos\\George Hanson\"\n",
    "images_aug = [seq.augment_image(img) for _ in range(10)]\n",
    "\n",
    "# Save augmented images\n",
    "for i, image_aug in enumerate(images_aug):\n",
    "    cv2.imwrite(f\"{f}/image_aug__{i}.jpg\", image_aug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c8ef189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "desired_size = (256, 256) # for example\n",
    "\n",
    "for filename in os.listdir(r'C:\\Users\\annem\\OneDrive\\Desktop\\photos\\t\\Edith Hanson Storms'):\n",
    "    img = cv2.imread(os.path.join(r'C:\\Users\\annem\\OneDrive\\Desktop\\photos\\t\\Edith Hanson Storms', filename))\n",
    "    if img is not None:\n",
    "        old_size = img.shape[:2]\n",
    "        ratio = float(desired_size[0])/max(old_size)\n",
    "        new_size = tuple([int(x*ratio) for x in old_size])\n",
    "        # resize the image\n",
    "        img = cv2.resize(img, (new_size[1], new_size[0]))\n",
    "        # create a new image with the desired size and fill it with white pixels\n",
    "        new_img = np.ones((desired_size[0], desired_size[1], 3), dtype=np.uint8) * 255\n",
    "        # calculate the center offset of the image\n",
    "        center_offset = ((desired_size[0]-new_size[0])//2, (desired_size[1]-new_size[1])//2)\n",
    "        # copy the resized image into the center of the new image\n",
    "        new_img[center_offset[0]:center_offset[0]+new_size[0], center_offset[1]:center_offset[1]+new_size[1]] = img\n",
    "        # save the new image with the same filename in the folder\n",
    "        cv2.imwrite(os.path.join('C:/Users/annem/OneDrive/Desktop/photos/t/Edith Hanson Storms/', filename), new_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be58c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Edith Hanson Storms\\1915~Edith WWSII Till WWSI.jpg_face2.jpg\")\n",
    "cv2.imshow('im',f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c4580",
   "metadata": {},
   "source": [
    "# CNN Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b46236",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 [==============================] - 6s 606ms/step - loss: 382.4240 - accuracy: 0.0687 - val_loss: 5.9296 - val_accuracy: 0.1159\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 5s 568ms/step - loss: 3.9288 - accuracy: 0.2176 - val_loss: 1.6734 - val_accuracy: 0.7246\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 5s 586ms/step - loss: 1.9772 - accuracy: 0.4695 - val_loss: 0.9819 - val_accuracy: 0.8696\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 4s 459ms/step - loss: 0.7975 - accuracy: 0.8130 - val_loss: 0.6378 - val_accuracy: 0.8406\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 4s 479ms/step - loss: 0.7964 - accuracy: 0.8435 - val_loss: 0.4188 - val_accuracy: 0.8986\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 5s 524ms/step - loss: 0.7662 - accuracy: 0.8397 - val_loss: 0.4495 - val_accuracy: 0.8841\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 4s 464ms/step - loss: 1.4924 - accuracy: 0.7672 - val_loss: 0.4790 - val_accuracy: 0.8986\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 4s 462ms/step - loss: 0.5836 - accuracy: 0.8550 - val_loss: 0.3341 - val_accuracy: 0.9420\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 4s 475ms/step - loss: 0.3734 - accuracy: 0.8931 - val_loss: 0.3161 - val_accuracy: 0.9275\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 4s 466ms/step - loss: 0.2557 - accuracy: 0.9275 - val_loss: 0.2968 - val_accuracy: 0.9565\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 5s 577ms/step - loss: 0.2430 - accuracy: 0.9275 - val_loss: 0.3782 - val_accuracy: 0.9565\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 5s 577ms/step - loss: 0.4657 - accuracy: 0.9427 - val_loss: 1.0281 - val_accuracy: 0.7246\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 4s 485ms/step - loss: 0.8489 - accuracy: 0.8168 - val_loss: 0.4126 - val_accuracy: 0.9130\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 5s 543ms/step - loss: 0.3652 - accuracy: 0.9008 - val_loss: 0.4427 - val_accuracy: 0.9420\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 5s 526ms/step - loss: 0.1438 - accuracy: 0.9542 - val_loss: 1.3006 - val_accuracy: 0.8841\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 4s 469ms/step - loss: 0.8405 - accuracy: 0.8969 - val_loss: 0.2651 - val_accuracy: 0.9275\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 5s 611ms/step - loss: 0.6265 - accuracy: 0.8626 - val_loss: 0.4235 - val_accuracy: 0.9565\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 5s 575ms/step - loss: 0.3813 - accuracy: 0.9198 - val_loss: 0.2362 - val_accuracy: 0.9710\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 5s 568ms/step - loss: 0.2819 - accuracy: 0.9313 - val_loss: 0.2138 - val_accuracy: 0.9710\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 5s 496ms/step - loss: 0.1449 - accuracy: 0.9733 - val_loss: 0.1162 - val_accuracy: 0.9710\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 5s 552ms/step - loss: 0.1339 - accuracy: 0.9924 - val_loss: 0.1328 - val_accuracy: 0.9710\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 4s 474ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.2007 - val_accuracy: 0.9710\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 5s 560ms/step - loss: 0.1209 - accuracy: 0.9924 - val_loss: 0.1830 - val_accuracy: 0.9710\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 5s 541ms/step - loss: 0.0587 - accuracy: 0.9962 - val_loss: 0.1830 - val_accuracy: 0.9710\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 4s 452ms/step - loss: 0.0691 - accuracy: 0.9847 - val_loss: 0.1618 - val_accuracy: 0.9710\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 5s 548ms/step - loss: 0.0236 - accuracy: 0.9962 - val_loss: 0.1601 - val_accuracy: 0.9710\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 4s 476ms/step - loss: 0.0182 - accuracy: 0.9924 - val_loss: 0.1520 - val_accuracy: 0.9710\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 4s 456ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9710\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 5s 600ms/step - loss: 0.0388 - accuracy: 0.9924 - val_loss: 0.0857 - val_accuracy: 0.9710\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 5s 548ms/step - loss: 0.0150 - accuracy: 0.9962 - val_loss: 0.0577 - val_accuracy: 0.9710\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 5s 545ms/step - loss: 0.0197 - accuracy: 0.9962 - val_loss: 0.1253 - val_accuracy: 0.9710\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 5s 517ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.1371 - val_accuracy: 0.9710\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 4s 464ms/step - loss: 0.0325 - accuracy: 0.9962 - val_loss: 0.1351 - val_accuracy: 0.9710\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 5s 547ms/step - loss: 0.0143 - accuracy: 0.9962 - val_loss: 0.1429 - val_accuracy: 0.9710\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 5s 538ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.1378 - val_accuracy: 0.9710\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 5s 497ms/step - loss: 0.0048 - accuracy: 0.9962 - val_loss: 0.1737 - val_accuracy: 0.9710\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 5s 571ms/step - loss: 0.0109 - accuracy: 0.9962 - val_loss: 0.1660 - val_accuracy: 0.9710\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 5s 578ms/step - loss: 0.0105 - accuracy: 0.9962 - val_loss: 0.1888 - val_accuracy: 0.9710\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 5s 586ms/step - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.1872 - val_accuracy: 0.9710\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 5s 509ms/step - loss: 0.0238 - accuracy: 0.9962 - val_loss: 0.2143 - val_accuracy: 0.9710\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 5s 526ms/step - loss: 0.0264 - accuracy: 0.9962 - val_loss: 0.2192 - val_accuracy: 0.9710\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 5s 605ms/step - loss: 0.0322 - accuracy: 0.9924 - val_loss: 0.2106 - val_accuracy: 0.9855\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 5s 603ms/step - loss: 0.1464 - accuracy: 0.9847 - val_loss: 0.3070 - val_accuracy: 0.9710\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 5s 581ms/step - loss: 0.5790 - accuracy: 0.9237 - val_loss: 0.5392 - val_accuracy: 0.8986\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 5s 563ms/step - loss: 0.4068 - accuracy: 0.9427 - val_loss: 0.4449 - val_accuracy: 0.9130\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 5s 538ms/step - loss: 0.2274 - accuracy: 0.9351 - val_loss: 0.2700 - val_accuracy: 0.9420\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 4s 497ms/step - loss: 0.2184 - accuracy: 0.9542 - val_loss: 0.3364 - val_accuracy: 0.9275\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 6s 619ms/step - loss: 0.2737 - accuracy: 0.9580 - val_loss: 0.3177 - val_accuracy: 0.9710\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 4s 494ms/step - loss: 0.2697 - accuracy: 0.9847 - val_loss: 0.3086 - val_accuracy: 0.9565\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 5s 562ms/step - loss: 0.5766 - accuracy: 0.9466 - val_loss: 0.2610 - val_accuracy: 0.9710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17233fcbaf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import pickle\n",
    "\n",
    "# Define function to load dataset\n",
    "def load_dataset(path):\n",
    "    X, y = [], []\n",
    "    for foldername in os.listdir(path):\n",
    "        folderpath = os.path.join(path, foldername)\n",
    "        for filename in os.listdir(folderpath):\n",
    "            imgpath = os.path.join(folderpath, filename)\n",
    "            img = cv2.imread(imgpath)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "            img = cv2.resize(img, (256, 256))\n",
    "           # print(img.shape)# Resize image\n",
    "            X.append(img)\n",
    "            y.append(foldername)\n",
    "           #     # Convert data to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # Encode labels\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(y)\n",
    "    y = np_utils.to_categorical(y)\n",
    "    # Save the encoder to a file\n",
    "  #  encoder.save('encoder.h5')\n",
    "    with open('encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "    # Return data and labels\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# # Load dataset\"C:\\Users\\annem\\OneDrive\\Desktop\\photos\\train\"\n",
    "X_train, y_train = load_dataset(r'C:\\Users\\annem\\OneDrive\\Desktop\\cnn\\train')\n",
    "X_test, y_test = load_dataset(r'C:\\Users\\annem\\OneDrive\\Desktop\\cnn\\test')\n",
    "\n",
    "# Define CNN model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(256, 256, 3)))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model to a file\n",
    "# with open('model.pkl', 'wb') as f:\n",
    "#     pickle.dump(model, f)\n",
    "#model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a10a200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<KerasTensor: shape=(None, 127, 127, 32) dtype=float32 (created by layer 'max_pooling2d')>, <KerasTensor: shape=(None, 125, 125, 64) dtype=float32 (created by layer 'conv2d_1')>, <KerasTensor: shape=(None, 62, 62, 64) dtype=float32 (created by layer 'max_pooling2d_1')>, <KerasTensor: shape=(None, 60, 60, 128) dtype=float32 (created by layer 'conv2d_2')>, <KerasTensor: shape=(None, 30, 30, 128) dtype=float32 (created by layer 'max_pooling2d_2')>, <KerasTensor: shape=(None, 115200) dtype=float32 (created by layer 'flatten')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'dense')>, <KerasTensor: shape=(None, 256) dtype=float32 (created by layer 'dropout')>, <KerasTensor: shape=(None, 17) dtype=float32 (created by layer 'dense_1')>]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs=[layer.output for layer in model.layers[1:]]\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ef273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ae034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9b98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fd65e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.7.0.72-cp37-abi3-win_amd64.whl (38.2 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\annem\\anaconda3\\envs\\gnn\\lib\\site-packages (from opencv-python) (1.19.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.7.0.72\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62d378b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "['Ed Beck', 'Ed Beck', 'Ed Beck']\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model and label encoder\n",
    "#model = pickle.load(open('model.pkl', 'rb'))\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "#model=load_model('model.h5')\n",
    "le = pickle.load(open('encoder.pkl', 'rb'))\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\from_SD\\1915~ Ede WWSII Paul Mathilda in Washington Park.jpg\")\n",
    "# Predict the name of the person in a test image\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\cnn\\train\\Winslow Page Storms\\Winslow Page Storms  from Storms_album.jpg\")\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\from_SD\\Schwartz Anna and Chris her son Storms_album.jpg\")\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\from_SD\\photo1 (10).jpg\")\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\from_SD\\1921 11 WWS p1 Death Claims Pythian.jpg\")\n",
    "#img = cv2.imread(r'C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\sd\\#44_WashingtonPark_Fall_1913.jpg')\n",
    "img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\sd\\#55 Edith WWII Til Uncle Paul (2).jpg\")\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\from_SD\\#63 1911 WWStorms Election Ad.jpg\")\n",
    "##img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\photos\\train\\William Wallace storms\\wwstorms.jpg\")\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\from_SD\\1939~Elinor Jean Storms possibly a college portrait.jpg\")\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\from_SD\\#63 1911 WWStorms Election Ad.jpg\")\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\from_SD\\1892~ W W Storms portrat by Ridgeway - Madison.jpg\")\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\photos\\train\\Ana Marie Thompson Hanson\\image_aug_3.jpg\")\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\from_SD\\Winslow Page Storms and Melissa Meacham Storms Storms_album.jpg\")\n",
    "#img=cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\from_SD\\1920~ Paul Matson.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#img_copy=img.copy()\n",
    "#img_copy = cv2.resize(img_copy, (256, 256))\n",
    "#img = cv2.resize(img, (500, 500))\n",
    "#img = np.expand_dims(img, axis=0)\n",
    "#pred = model.predict(img)\n",
    "##pred_label = np.argmax(pred)\n",
    "#name=le.inverse_transform([pred_label])[0]\n",
    "# Draw bounding box around detected face\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "names=[]\n",
    "#faces=cv2.resize(faces, (256,256))\n",
    "#print(len(faces))\n",
    "for (x,y,w,h) in faces:\n",
    "    # Extract the face ROI and preprocess it for the model\n",
    "    roi = gray[y:y+h, x:x+w]\n",
    "    roi = cv2.resize(roi, (256, 256))\n",
    "   \n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_GRAY2RGB)\n",
    "    #roi = np.expand_dims(roi, axis=0)\n",
    "    #roi = np.expand_dims(roi, axis=3)\n",
    "    roi = roi / 255.0\n",
    "    #print(roi.shape)\n",
    "    # Perform prediction for the face ROI\n",
    "    pred = model.predict(np.expand_dims(roi, axis=0))\n",
    "    pred_label = np.argmax(pred)\n",
    "    name = le.inverse_transform([pred_label])[0]\n",
    "    names.append(name)\n",
    "    # Draw bounding box and label for the face\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    cv2.putText(img, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)\n",
    "\n",
    "# Display the image with bounding boxes and labels for each face\n",
    "#im=cv2.resize(img, (250,250))\n",
    "cv2.imshow('Image', img)\n",
    "print(names)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947635cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c886fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf4947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77312c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10790871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e595c22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Load the trained model and label encoder\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#model = pickle.load(open('model.pkl', 'rb'))\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m=\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m le \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Predict the name of the person in a test image\u001b[39;00m\n\u001b[0;32m      7\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mannem\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFace Recognition\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msd\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m#44_WashingtonPark_Fall_1913.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# Load the trained model and label encoder\n",
    "#model = pickle.load(open('model.pkl', 'rb'))\n",
    "model=load_model('model.h5')\n",
    "le = pickle.load(open('encoder.pkl', 'rb'))\n",
    "# Predict the name of the person in a test image\n",
    "img = cv2.imread(r'C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\sd\\#44_WashingtonPark_Fall_1913.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_copy=img.copy()\n",
    "#img = cv2.resize(img, (256, 256))\n",
    "img = np.expand_dims(img, axis=0)\n",
    "pred = model.predict(img)\n",
    "pred_label = np.argmax(pred)\n",
    "name=le.inverse_transform([pred_label])[0]\n",
    "# Draw bounding box around detected face\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "gray = cv2.cvtColor(img_copy, cv2.COLOR_RGB2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "faces = cv2.resize(faces, (256, 256))\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img_copy,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    cv2.putText(img_copy, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)\n",
    "\n",
    "# Display image with name and bounding box\n",
    "cv2.imshow('Image', img_copy)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4696bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cde9bc86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 64, 64, 3), found shape=(None, 1, 64, 64, 1, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m roi \u001b[38;5;241m=\u001b[39m roi \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Perform prediction for the face ROI\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m pred_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(pred)\n\u001b[0;32m     30\u001b[0m name \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39minverse_transform([pred_label])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileuwr2tak6.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\annem\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 64, 64, 3), found shape=(None, 1, 64, 64, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model and label encoder\n",
    "#model = pickle.load(open('model.pkl', 'rb'))\n",
    "model=load_model('model.h5')\n",
    "le = pickle.load(open('encoder.pkl', 'rb'))\n",
    "\n",
    "# Load the test image\n",
    "#img = cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\photos\\train\\Anna Caspersen Schwartz Thompson\\Anna Caspersen Schwartz Thompson.jpg\")\n",
    "img = cv2.imread(r'C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\sd\\#44_WashingtonPark_Fall_1913.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_copy = img.copy()\n",
    "\n",
    "# Detect faces in the image\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "gray = cv2.cvtColor(img_copy, cv2.COLOR_RGB2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "# Iterate through the detected faces and perform prediction for each face\n",
    "for (x,y,w,h) in faces:\n",
    "    # Extract the face ROI and preprocess it for the model\n",
    "    roi = gray[y:y+h, x:x+w]\n",
    "    roi = cv2.resize(roi, (64, 64))\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_GRAY2RGB)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "    roi = np.expand_dims(roi, axis=3)\n",
    "    roi = roi / 255.0\n",
    "    \n",
    "    # Perform prediction for the face ROI\n",
    "    pred = model.predict(np.expand_dims(roi, axis=0))\n",
    "    pred_label = np.argmax(pred)\n",
    "    name = le.inverse_transform([pred_label])[0]\n",
    "    \n",
    "    # Draw bounding box and label for the face\n",
    "    cv2.rectangle(img_copy,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    cv2.putText(img_copy, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)\n",
    "\n",
    "# Display the image with bounding boxes and labels for each face\n",
    "cv2.imshow('Image', img_copy)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30579e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c186e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(612, 493, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = cv2.imread(r\"C:\\Users\\annem\\OneDrive\\Desktop\\photos\\train\\Anna Caspersen Schwartz Thompson\\Anna Caspersen Schwartz Thompson.jpg\")\n",
    "img2 = cv2.imread(r'C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\sd\\#44_WashingtonPark_Fall_1913.jpg')\n",
    "img1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f82eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1168, 1517, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acdc8ad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<1,-1,-1>,struct cv::impl::A0x981fb336::Set<3,4,-1>,struct cv::impl::A0x981fb336::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Resize the ROI to match the input size of the model\u001b[39;00m\n\u001b[0;32m     32\u001b[0m roi \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(roi, (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n\u001b[1;32m---> 33\u001b[0m roi \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_GRAY2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Apply histogram equalization to improve contrast\u001b[39;00m\n\u001b[0;32m     35\u001b[0m roi \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mequalizeHist(roi)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<1,-1,-1>,struct cv::impl::A0x981fb336::Set<3,4,-1>,struct cv::impl::A0x981fb336::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import pickle\n",
    "# Load the trained model and label encoder\n",
    "model = load_model('model.h5')\n",
    "le = pickle.load(open('encoder.pkl', 'rb'))\n",
    "\n",
    "# Load the image\n",
    "img = cv2.imread(r'C:\\Users\\annem\\OneDrive\\Desktop\\Face Recognition\\sd\\#44_WashingtonPark_Fall_1913.jpg')\n",
    "\n",
    "# Convert image to grayscale and detect faces\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# #img_copy=img.copy()\n",
    "# img = cv2.resize(img, (64, 64))\n",
    "# img = np.expand_dims(img, axis=0)\n",
    "faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "\n",
    "# Loop over detected faces and classify each one\n",
    "for (x, y, w, h) in faces:\n",
    "    # Extract the ROI where the face is located\n",
    "    roi = gray[y:y+h, x:x+w]\n",
    "    # Resize the ROI to match the input size of the model\n",
    "    roi = cv2.resize(roi, (64, 64))\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_GRAY2RGB)\n",
    "    # Apply histogram equalization to improve contrast\n",
    "    roi = cv2.equalizeHist(roi)\n",
    "    # Reshape the ROI to have a single channel (grayscale)\n",
    "    roi = np.expand_dims(roi, axis=-1)\n",
    "    # Reshape the ROI to have a batch dimension (needed for model.predict)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "    # Normalize the pixel values to be between 0 and 1\n",
    "    roi = roi / 255.0\n",
    "    # Apply the trained model to the ROI to classify the person\n",
    "    pred = model.predict(roi)\n",
    "    pred_label = np.argmax(pred)\n",
    "    name = le.inverse_transform([pred_label])[0]\n",
    "    # Draw a bounding box around the face and display the name\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cv2.putText(img, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with bounding boxes and names\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83c48e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202 images belonging to 17 classes.\n",
      "Found 202 images belonging to 17 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the directories for the training data\n",
    "train_dir = r'C:\\Users\\annem\\OneDrive\\Desktop\\photos'\n",
    "#validation_dir = 'path/to/validation/directory'\n",
    "test_dir = r'C:\\Users\\annem\\OneDrive\\Desktop\\photos'\n",
    "\n",
    "# Set up the data augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Set up the image generators\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    classes=['Ana Marie Thomspson Hanson', 'Ana Caspersen Schwartz Thompson', 'Aunt Kate', 'Chris Schwarz', 'Ed Beck', 'Edith Hanson Storms', 'Eleanor Nora Storms', 'Elinor Jean Storms Danford', 'Florence Penn', 'George Hanson', 'Hans Isaac Hanson', 'Paul Matson', 'Persis Melissa Meacham Storms', 'william wallace', 'William Wallace storms', 'William Wallace Storms II', 'Winslow Page Storms']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    classes=['Ana Marie Thomspson Hanson', 'Ana Caspersen Schwartz Thompson', 'Aunt Kate', 'Chris Schwarz', 'Ed Beck', 'Edith Hanson Storms', 'Eleanor Nora Storms', 'Elinor Jean Storms Danford', 'Florence Penn', 'George Hanson', 'Hans Isaac Hanson', 'Paul Matson', 'Persis Melissa Meacham Storms', 'william wallace', 'William Wallace storms', 'William Wallace Storms II', 'Winslow Page Storms']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4d14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fc5e9b5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_width' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m---> 12\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[43mimg_width\u001b[49m, img_height, \u001b[38;5;241m3\u001b[39m)))\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39madd(MaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv2D(\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_width' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f380ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ba944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af53ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
